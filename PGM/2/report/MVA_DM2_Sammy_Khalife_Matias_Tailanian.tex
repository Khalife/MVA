%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}  
\usepackage[T1]{fontenc} 
\usepackage[top=2cm,bottom=2cm,left=1.3cm,right=1cm,asymmetric]{geometry}

\usepackage{amsfonts}
\usepackage{fancyhdr}
\usepackage{array,multirow}
% \usepackage{array,multirow,makecell}
\usepackage{amsmath}

\usepackage{cancel}
\usepackage{subfig}
\usepackage{wrapfig}
\usepackage[table]{xcolor}
\usepackage{multirow}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

% \setcellgapes{1pt}
% \makegapedcells
% \newcolumntype{R}[1]{>{\raggedleft\arraybackslash }b{#1}}
% \newcolumntype{L}[1]{>{\raggedright\arraybackslash }b{#1}}
% \newcolumntype{C}[1]{>{\centering\arraybackslash }b{#1}}

\pagestyle{fancy}
\renewcommand{\footrulewidth}{1pt}
\fancyhead[R]{\textit{Master MVA : Graphical models}}
\fancyfoot[L]{\textit{}}
%\usepackage{unicode-math}
%\setmathfont{XITS Math}
%\setmathfont[version=setB,StylisticSet=1]{XITS Math}


%\geometry{hmargin=1.5cm,vmargin=2cm}   

\begin{document}


\section*{Sammy Khalife \& Matias Tailanian}
\subsubsection*{29/10/2014}
\subsubsection*{Assignment 2}
\section*{1 Distributions factorizing in a graph} 
\subsection*{a.}
- Given the definition of $p \in L(G)$, we have for any x :
\begin{eqnarray*}
p(x) & = & \prod_{k=1}^{n}p(x_{k}|x_{\pi_{k}})\\
& = &  \prod \limits^n_{\substack{k=1 \\ k \neq i k \neq j} }p(x_{k}|x_{\pi_{k}}) \; p(x_{i}|x_{\pi_{i}})p(x_{j}|x_{\pi_{i}},x_{i})
\end{eqnarray*}
Since by assumption, $\pi_{j}=\pi_{i} \cup {i}$: $p(x_j|x_{\pi_j})=p(x_j|x_{\pi_{i\cup i}})=p(x_j|x_{\pi_i},x_i)$.~\\
Thanks to the Bayes formula applied to $p(x_{j}|x_{\pi_{i}}, x_{i})$ :
\begin{eqnarray*}
p(x) & = & \prod \limits_{\substack{k=1 \\ k \neq i k \neq j} } p(x_{k}|x_{\pi_{k}})        
\cancel {p(x_{i}|x_{\pi_{i}})}
p(x_{i}|x_{\pi_{i}},x_{j})\frac{p(x_{j}|x_{\pi_{i}})}{\cancel{p(x_{i}|x_{\pi_{i}})}}\\
& = &  \prod \limits_{\substack{k=1 \\ k \neq i k \neq j} } p(x_{k}|x_{\pi_{k}})p(x_{i}|x_{\pi_{i}},x_{j})p(x_{j}|x_{\pi_{i}})
\end{eqnarray*}
This last equality is saying that $p$ is factorizing in $G^{'}=(V,E^{'})$, with $E^{'} = (E \setminus \{ i \mapsto j \}\cup\{j\mapsto i\})$. ~\\
Then $L(G) \subset L(G')$.~\\
By going back in the equalities, we actually have $L(G^{'}) \subset L(G)$.
Hence, $L(G)=L(G^{'})$.
~\\
~\\
- As $G$ is a directed tree, it's also a \emph{graph}. Let us denote the root node without parents $x_{n}$. ~\\
p factorizes in $L(G)$:
$$p(x) = \prod^n_{i=1}p(x_i|x_{\pi_i}) = [\: \prod^{n-1}_{i=1}p(x_i|x_{\pi_i}) \: ] \:p(x_{n})$$
As $G$ is a directed tree with no \emph{v-structures}, it has the same quantity of \emph{edges} as \emph{clicks}: $n-1$. Furthermore each click has two elements, i.e. sets parent-children.~\\
~\\
Then we can define for the sons in the (n-2) clicks that do not contain the root : $$\psi_{c}(x_{c}) = \underbrace{(n-1)^{\frac{1}{n-1}}p(x_c|x_{\pi_c})}_{\text{Depending only on the elements of the clique}}$$~\\
and for the clique containing the root : 
%$\psi_{c}(x_{c})=\begin{cases} 
%(n-1)^{\frac{1}{n-1}}p(x_{n}|x_{\pi_{n}}) & \text{if  }  x_{c}=x_{n} \\
%p(x_{n}) & \text{otherwise } 
%\end{cases}$
$\psi_{c}(x_{c})=\underbrace{(n-1)^{\frac{1}{n-1}}p(x_{n}|x_{\pi_{n}})p(x_{n})}_{\text{Depending only on the elements of the clique}}$
~\\

With $Z=n-1$ (the number of cliques), we obtain:
$$p(x) = \frac{1}{Z}\prod_{c\in \mathcal{C}}\psi_c(x_c)$$
where $\mathcal{C}$ is the set of clicks of $G$. This expression corresponds to a factorization in the symetrized graph $G^\prime$ of $G$, which has $Z=n-1$ clicks of cardinal $2$, $\{x_k, x_{\pi_k}\}$, proving: $\mathcal{L}(G)\subseteq \mathcal{L}(G^\prime)$.
~\\
~\\
In the other hand, as $E^\prime \subseteq E$, as a general result on graphical models : $\mathcal{L}(G^\prime)\subseteq \mathcal{L}(G)$.
Finally, $$\boxed{\mathcal{L}(G) = \mathcal{L}(G^\prime)}$$
%TODO: doubt. (n-1) inside the prod?

\subsection*{b.}
First we know that for any directed graph $G$, and its symmetrized graphed $G'$, we always have $L(G)\subset L(G')$.~\\
Then, we just have to discuss the case $L(G') \subset  L(G)$.~\\
~\\
For a 2 nodes-graph, if there is an edge. we always have for p $\in L(G'), p(x)=p(x2|x1)p(x1)$, this corresponds to a trival directed graph. If there is no edge, then the directed graph with no edge corresponds to the same factorization of p in the symmetrized graph.~\\
~\\
For a 3 nodes-graph :~\\
- No-edges graph : all the nodes are independant, then the factorization is the same in the directed graph with no no edges.~\\
- One-edge graph : there is only one clique containing more than one node (2 nodes)  in the undirected graph, with a factorization :~\\
$$p(x) = [\prod^n \limits_{\substack{i=1 \\ i \neq i_{0} i \neq {i_{1}} } }\psi_{i}(x_i)] \: \psi_{i_{0},i_{1}}(x_{i_{0}},x_{i_{1}})$$
This corresponds to a factorization in a directed graph with the same edge oriented in any direction.~\\
- 2 edges-graph : Then, the graph is an undirected tree with no v-structures, and we use the result of 1.b to have $L(G)=L(G')$.~\\
~\\
\begin{wrapfigure}{r}{0.4\textwidth}
	\vspace{-40pt}
	\begin{center}
		\includegraphics[width=0.3\textwidth]{./pics/counterexample.png}
	\end{center}
	\vspace{-20pt}
	\caption{}
	\label{fig:counterexample}
\end{wrapfigure}
Let's consider a 4 nodes graph like the one shown at Figure \ref{fig:counterexample}:

%$$p(x)=\psi_{1}(x_{1},x_{2})\psi_{2}(x_{2}, x_{3}) \psi_{3}(x_{3}, x_{4})\psi_{4}(x_{4}, x_{1})$$

~\\
A directed graph should have the same edges, otherwise some independances would be erased or added (intuitive statement).
Moreover, there must be a v-structure, otherwise the directed graph would be cyclic and not be a DAG. Let's suppose this v-structure is on the node 1.
Then, 2 is not independant of 4 given (1,3) since there is a v-structure in (1,3), whereas in the symmetrized graph, 2 is independant of 4 given (1,3).  
This shows that p is in L(G) but not in L(G').~\\
The smallest undirected graph G' such that there is no directed graph G with L(G')=L(G) contains 4 nodes.


\section*{2 d-separation}
\subsection*{a.}
Let us denote $\mathbf{G_M}=(E, V_{M})$ the moral graph of $\mathbf{G}$, and let us suppose that S separates A and B in $\mathbf{G_M}$.~\\
~\\
Then, every chain going from A to B goes through a node in S, in $\mathbf{G_M}$ and in $\mathbf{G}$ . ~\\
Let c be a chain in $\mathbf{G}$, then it goes through at least one node in S.~\\
-If there is at least 2 nodes of the chain that are in S, there exists a non v-structure in S (indeed, two consecutive nodes cannot form both a v-structure), then the chain is blocked.~\\
-Otherwise, if the chain contains only one node d in S, with $d_{-} \in A$ and $d_{+} \in B$, we show that $(d_{-},,d,d,_{+})$ cannot be a v-structure. If it was, then $(d_{-}, d_{+}) \in V_{M}$, and would be a chain in the moral graph that does not go through S : this is a contradiction.~\\
A and B are d-separated by S in $\mathbf{G}$.


\subsection*{b.}
This property of transitivity for d-separation is false. 2 counterexamples are shown in the figure below.
\begin{figure} [h!]
\centering
  \subfloat[Simple DAG : T separates A and S but contains a v-structure in a chain going from A to B]{\label{fig:dagSimple} 
  		\includegraphics[width=.35\textwidth]{./pics/dagSimple.png}} \hspace{30pt}
  \subfloat[Complex DAG]{\label{fig:dagComplex} 
  		\includegraphics[width=.4\textwidth]{./pics/dagComplex.png}} \\
  \caption{Counterexamples}
  \label{fig:ex2_dags}
  \vspace{-20pt}
\end{figure}

\subsection*{c.}
\begin{wrapfigure}{r}{0.4\textwidth}
	\vspace{-20pt}
	\begin{center}
		\includegraphics[width=0.3\textwidth]{./pics/ex2c.png}
	\end{center}
	\vspace{-20pt}
	\caption{Graph $G$}
	\label{fig:ex2c}
	\vspace{-30pt}
\end{wrapfigure}
Given the graph shown in Figure \ref{fig:ex2c}, we consider the following statements:
\begin{enumerate}
	\item $X_{\{1,2\}}\independent X_4|X_3$
	\item $X_{\{1,2\}}\independent X_4|X_5$
	\item $X_1\independent X_6|X_{\{2,4,7\}}$
\end{enumerate}
1-TRUE.~\\
First we can see that $A=\{1,2\}$ and $B=\{4\}$ are d-separated by $C=\{3\}$. Indeed the chains going from A to B are exactly $(1,8,4)$ and $(2,8,4)$. Given that $\{8\}$ is a v-structure which is not in C, and C contains no descendants of $\{8\}$, these chains are locked in $\{8\}$, the statement 1 is true thanks to the Global Markov property.~\\
~\\
2-FALSE.
We can see that $A=\{1,2\}$ and $B=\{4\}$ are not d-separated by $C=\{5\}$, since the only chain going from A to B contains $\{7\}$ which is not a V-structure, and is not in C.~\\
~\\
3-TRUE.~\\
We show that $A=\{1\}$ and $B=\{6\}$ are d-separated by $C=\{2,4,7\}$. Indeed, the only chain going from A to B is $\{1,8,7,6\}$, and this chain is blocked in 7 since $7\in C$ and $\{8, 7, 6\}$ is not a v-structure. Given $p\in L(G)$, it verifies the Global Markov property, i.e $X_{A} \bot X_{B} | X_{C}$. The statement is true.~\\

\section*{Implementation - Gaussian Mixtures}
\subsection*{a.}
In Figure \ref{fig:3a_km} is shown the behavior of the K-Means algorithm for two different random initializations. In different colors are represented the different clusters found. The triangles shows the evolution of the mean of each cluster. The algorithm was ran several times and in all cases reached satisfactory results, resulting in very similar values for $\mu_i$ at the end. Furthermore, the distortion of the clustering is invariant to the random initialization of the means.
\begin{figure} [h!]
\centering
  \subfloat[Random initialization 1]{\label{fig:3a} 
  		\includegraphics[width=.5\textwidth]{./pics/3a.pdf}} 
  \subfloat[Random initialization 2]{\label{fig:3a2} 
  		\includegraphics[width=.5\textwidth]{./pics/3a2.pdf}} \\
  \caption{K-Means algorithm. In different colors are shown the different clusters found at the end of the algorithm. Also, with triangles is shown the evolution of the mean of each cluster, with corresponding color.} 
  \label{fig:3a_km}
\end{figure}

Defining as a distortion measure $D$:
$$ D = \sum\limits_{i=1}^{N}\sum\limits_{j=1}^{c}k_j^i||x_i-\mu_j||_2 $$
where $k_j^i$ takes value 1 for $j$ corresponding to the assigned class of the sample $i$ and 0 otherwise, we can compute a measure that gives an idea of how well the algorithm performed. As can be seen in table \ref{tab:kmeansdistortion}, for different realizations (random initializations) the value of $D$ remains almost unchanged, which means that no matter where the algorithm is initialized, the result after some iterations is almost the same, leading to a robust algorithm. The slight differences of the final values for the distortion can be explained by possible local minimas reached, since the problem is not convex.
\begin{table}[h!]
\centering
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|} 
	\hline
	\cellcolor[gray]{0.7} 1 & \cellcolor[gray]{0.7} 2 & \cellcolor[gray]{0.7} 3 & \cellcolor[gray]{0.7} 4 & \cellcolor[gray]{0.7} 5 & \cellcolor[gray]{0.7} 6 & \cellcolor[gray]{0.7} 7 & \cellcolor[gray]{0.7} 8 & \cellcolor[gray]{0.7} 9 & \cellcolor[gray]{0.7} 10 \\ \hline
	1108.46 & 1108.46 & 1103.50 & 1102.55 & 1109.42 & 1109.42 & 1109.42 & 1103.92 & 1102.85 & 1109.42\\ \hline
	\end{tabular} 
	\caption{Distortion measure for k-Means}
	\label{tab:kmeansdistortion}
\end{table}

In the other hand, in Figure \ref{fig:3aLejos_km} are shown results of the k-Means algorithm not so satisfactory. In this case the initialization of the means was made far away from the data on purpose, to analyze the behavior of the algorithm. As can be seen, with random initializations far away from data the algorithm not always finds a proper solution, as can be seen in Figure \ref{fig:3aLejos2}.
\begin{figure} [h!]
\centering
  \subfloat[Far random initialization 1]{\label{fig:3aLejos} 
  		\includegraphics[width=.5\textwidth]{./pics/3aLejos.pdf}} 
  \subfloat[Far random initialization 2]{\label{fig:3aLejos2} 
  		\includegraphics[width=.5\textwidth]{./pics/3aLejos2.pdf}} \\
  \caption{K-Means algorithm with random initialization far away from data. In different colors are shown the different clusters found at the end of the algorithm. Also, with triangles is shown the evolution of the mean of each cluster, with corresponding color.} 
  \label{fig:3aLejos_km}
  \vspace{10pt}
\end{figure}

The distortion for the result shown in Figure \ref{fig:3aLejos} is 1109.42, and the distortion for the result shown in Figure \ref{fig:3aLejos2} is 1837.15. For the example in Figure \ref{fig:3aLejos} the results are as good as initializing the means close to the data, and distortion comparable to the ones shown in table \ref{tab:kmeansdistortion}.

\subsection*{b.}
In the Isotropic case, under the assumption that $\Sigma_{j} = \sigma Id$, the $\Sigma$ term in the log-likelyhood is just $N \log(\frac{1}{\sigma^{K}})-\frac{1}{2\sigma^{2}} \sum \limits_{i=1}^N ||x_{i}-\mu_{j}||^{2}$. Then maximizing with respect to $\sigma$ yields:
$$\sigma=\sqrt{\frac{1}{2KN}\sum \limits_{i,j} ||x_{i}-\mu_{j}||^2}$$
For the other parameters $\Pi$ and $\mu$, the updating formulas are the same as in the general case for $\Sigma$.\\

In Figure \ref{fig:3b} is shown the results for the Isotropic case. In different colors are shown the different clusters found. The triangles represent the means finally obtained, the colors of the samples represent the hidden variables, and in solid line an ellipse that contains the 90\% of the mass of each Gaussian. 
\begin{figure}[h!]
	\centering 
	\includegraphics[width=.7\textwidth]{./pics/3b.pdf}
	\caption{Expectation maximization: 90\% mass of the Gaussian}
	\label{fig:3b}
\end{figure}
To compute this ellipse, in the general case we consider the joint probability of the Gaussian:
$$ p(x,y) = \frac{1}{2\pi \sqrt{|\Sigma|}}\exp \left(-\frac{1}{2}(z-\mu)\Sigma^{-1}(z-\mu)^T\right)$$
The directions of the two axis of the ellipse are given by the eigenvectors of $\Sigma$, while the extent of the great and small radius are $r\sqrt{\lambda_1}$ and $r\sqrt{\lambda_2}$ respectively, being $\lambda_1$ the greatest eigenvalue and $\lambda_2$ the smallest, and $r$ defined as: $$r^2=(z-\mu)\Sigma^{-1}(z-\mu)^T$$
$\Sigma$ is assumed to be symmetric and positive definite, so the most general form of the matrix is:
$$\Sigma = \left[
\begin{array}{cc}
\sigma_1^2 & \rho\sigma_1\sigma_2 \\ 
\rho\sigma_1\sigma_2 & \sigma_2^2
\end{array} 
\right]$$
The ellipse can be parametrized as:
\begin{align}
\label{ec:par1}x & = r\sigma_1 \cos(\theta) +\mu_1 \\
\nonumber y & = r\sigma_2 (\rho \cos(\theta)+\sqrt{1-\rho^2}\sin(\theta) )+\mu_2
\end{align}
For the isotropic case, where $\rho=0$, the parametrization will be:
\begin{align}
\nonumber x & = r\sigma_1 \cos(\theta) +\mu_1 \\
\nonumber y & = r\sigma_2 \sin(\theta) +\mu_2
\end{align}
The cumulative function of the Gaussian is $$F(r) = 1-e^{-r^2/2}$$ So given the desired probability $p$ of a point to fall at a certain distance $r$ from the Gaussian, $r$ can be computed as 
\begin{equation}
	r= F^{-1}(p)=\sqrt{-2\ln (1-p)}
	\label{ec:r}
\end{equation}
Then, the generalize quantile parameter $r$ is given by this formula to have a probability $p$ that a point falls in the resulting $\theta$ parametrized ellipse.
\subsection*{c}
In the case of general form matrix $\Sigma$, we have the classical updating formulas for the EM algorithm. In Figure \ref{fig:3d} are shown the results for this case. As in the previous part the color of the samples represent the hidden variables, the triangles are the means and the solid line the ellipse which includes the 90\% of the mass of the Gaussian, in this case recalling the general case described in the previous section and following the equations \ref{ec:par1} and \ref{ec:r}.

\begin{figure}[h!]
	\centering 
	\includegraphics[width=.7\textwidth]{./pics/3c.pdf}
	\caption{Expectation maximization in the general case: 90\% mass of the Gaussian}
	\label{fig:3c}
\end{figure}

\subsection*{d}
The results over the test dataset are shown in Figure \ref{fig:3d}. Figure \ref{fig:3bTest} correspond to the isotropic case and Figure \ref{fig:3cTest} to the general case.
\begin{figure} [h!]
\centering
  \subfloat[Diagonal $\Sigma$]{\label{fig:3bTest} 
  		\includegraphics[width=.4\textwidth]{./pics/3bTest.pdf}} \hspace{20pt}
  \subfloat[General form $\Sigma$]{\label{fig:3cTest} 
  		\includegraphics[width=.35\textwidth]{./pics/3cTest.pdf}} \\
  \caption{Results on test data} 
  \label{fig:3d}
\end{figure}
As can be seen the obtained results are also very good for the test dataset. For the general case the big majority of samples fall inside the drawn ellipse, and the learned model fits the data very well.\\

A comparison of the final Likelihood obtained at the end of the algorithm is shown in Table \ref{tab:Likelihood}.
\begin{table}[h!]
\centering
	\begin{tabular}{c|c|c|} 
	\cline{2-3}
	& \multicolumn{1}{c|}{\cellcolor[gray]{0.7} \textbf{Train}}  
	& \multicolumn{1}{c|}{\cellcolor[gray]{0.7} \textbf{Test}}
	\\ \hline
	
	\multicolumn{1}{|c|}{\cellcolor[gray]{0.8} \textbf{Diagonal Sigma}}   & -2843.4680 & -2798.0809 \\ \hline
	\multicolumn{1}{|c|}{\cellcolor[gray]{0.8} \textbf{General form Sigma}}   & -2370.8841
	 & -2399.2991 \\ \hline
	
	\end{tabular} 
	\caption{Final Likelihood}
	\label{tab:Likelihood}
\end{table}
As can be seen, the Likelihood is greater for the general case in both datasets, as expected. Also it can be noticed that the performance enhances a bit in the test dataset comparing to the training one for the general case, which means that the learned model fits very well the true distribution of samples, as it performs a very good classification for new samples.
\end{document}